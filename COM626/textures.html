<!DOCTYPE html>
<html>
<head>
<title>OpenGL Textures and the Camera</title>
<link rel='stylesheet' type='text/css' href='../css/android.css' />
<style>
img {
    border: 1px solid black;
}
</style>
</head>
<body>
<div class='titlebox'>
<h1>Mobile Development and 3D Graphics - Part 11</h1>
<h1>OpenGL part 5 - Textures and the Camera</h1>
</div>
<h2>Introduction</h2>

<p>A <em>texture</em> is a raster (bitmap) image which is overlaid on a 3D shape in OpenGL and other 3D graphics APIs. Textures are typically used to apply realistic surfaces to 3D models and shapes, for example a brick texture can be overlaid on a wall, or a metallic texture could be overlaid on a robot model. Textures can be a range of formats including PNG: in a typical application we would load the texture from file and apply it to a model or 3D shape in our OpenGL world. In this module, however, we are going to particularly focus on how textures can be used in an augmented reality (AR) application, to show the camera feed within an OpenGL scene.</p>

<h3>Components of an augmented reality OpenGL application</h3>
<p>The following classes are key to an OpenGL AR application:
    <ul>
    <li>The GLSurfaceView with its Renderer, the same as for any other OpenGL application;</li>
    <li>The camera management code (CameraX here) (week 6)</li>
    <li>A <code>SurfaceTexture</code>. This is an object representing the actual     texture obtained from the camera feed. You create it in your OpenGL code and setup CameraX to stream the camera feed to it.</li>
    <li>Sensor-handling code (week 5), with the orientation matrix used as the view matrix (we will look at this next week)</li>
    </ul>
</p>

<h2>Creating a texture</h2>

<p>We first need to <em>create a texture</em>, then bind it to an on-GPU <em>texture unit</em> (see below) and, if we want to stream the camera feed into the texture, create a <em>SurfaceTexture</em> (also see below).</p>

<h3>Initialising a texture</h3>

<p>
In our OpenGL renderer, we need to <em>create a texture</em>.
We use <code>OpenGLUtils.genTexture()</code> to do this. This typically goes in the <code>onSurfaceCreated()</code>:
<pre>
val textureId = OpenGLUtils.genTexture()
</pre>
What this is doing is <em>generating a texture ID</em>. A texture ID is a unique ID number representing a particular texture. Before we use it further, we have to check it's not zero; if <code>OpenGLUtils.genTexture()</code> returns zero, it means it failed to generate the texture successfully.
</p>
<h3>Binding a texture to a texture unit</h3>
<p>The next step is to bind the texture to a specific on-GPU <em>texture unit</em>. A texture unit, of which more than one is available, is a specific piece of hardware which deals with processing textures. The diagram illustrates this:
<br />
<img src="../images/textureunits.png" alt="Texture units" />
<br />
 We can do this with <code>OpenGLUtils.bindTextureToTextureUnit()</code> as shown below. This code is also in <code>onSurfaceCreated()</code>.
<pre>
if (textureId != 0) {
    OpenGLUtils.bindTextureToTextureUnit(textureId, GLES20.GL_TEXTURE0, OpenGLUtils.GL_TEXTURE_EXTERNAL_OES)

    // further code continues here...
} 
</pre>
Note the <code>GL_TEXTURE0</code> refers to the first on-GPU texture unit (ref: <a href='https://www.khronos.org/opengles/sdk/1.1/docs/man/glActiveTexture.xml'>here</a>). Note that the texture unit - a piece of GPU hardware for processing textures - is <em>different</em> to the texture ID which is a CPU (Kotlin) based value representing the texture.  There are multiple texture units available, <code>GL_TEXTURE1</code> is the second, <code>GL_TEXTURE2</code> is the third, etc.
</p>
<p>The final argument is the texture type. Normally this will be <code>GLES20.GL_TEXTURE_2D</code> (e.g. if we are loading a texture from a file) but in the case of the camera feed, the format is a bit different so we have to use <code>OpenGLUtils.GL_TEXTURE_EXTERNAL_OES)</code>.</p>
<h3>Creating a SurfaceTexture</h3>
<p>Once we've created a texture and bound it to a texture unit, we then need to create a <code>SurfaceTexture</code> object, using the texture ID. This allows us to stream the camera feed into the texture.
<pre>surfaceTexture = SurfaceTexture(textureId)</pre>

<h2>Setting up textures on the shader</h2>

<p>Once we have initialised a texture, we need to specify how it will be applied to a shape.This is done on the GPU using shaders, as described below.</p>

<h3>S and T coordinates</h3>
<p>
Textures are defined using so-called S and T coordinates.
A texture is a 2D raster/bitmap image which is overlaid on a 3D shape. 
The S coordinate of the texture represents its horizontal axis, and the T
coordinate represents its vertical axis. The coordinates range from 0 to 1.
S=0, T=0 represents the bottom left. So:
    <ul>
    <li>S=0, T=0 : bottom left of texture;</li>
    <li>S=1, T=0 : bottom right of texture;</li>
    <li>S=1, T=1 : top right of texture;</li>
    <li>S=0, T=1 : top left of texture.</li>
    </ul>
When applying a texture to a 3D shape, we need to write a shader which
<em>relates x, y and z coordinates of a shape</em> to <em>the S and T coordinates of a texture</em>.
So we can specify what vertices of a shape the texture overlays. In the case of a camera feed, we need to create a <em>rectangle made up of two triangles</em>. This rectangle will occupy the whole screen and be flat, with no sense of depth and no perspective effect (no projection matrix applied). When this is the case, <strong>the x eye coordinate ranges from -1 (left of screen) to +1 (right of screen) and the y eye coordinate ranges from -1 (bottom of screen) to +1 (top of screen) with the origin in the centre. Furthermore z is always 0 as there is no sense of perspective.</strong> So the rectangle would have coordinates (-1,-1,0), (-1,1,0), (1,1,0) and (1,-1,0), and the following equation would relate <code>x</code> and <code>y</code> coordinates to <code>s</code> and <code>t</code> coordinates: 
<pre>
S coordinate = (X coordinate+1) / 2
T coordinate = (Y coordinate+1) / 2
</pre>
For example, for the vertex (-1, -1, 0):
<pre>
S coordinate = 0/2 = 0
T coordinate = 0/2 = 0
</pre>
and for the vertex (1, 1, 0):
<pre>
S coordinate = 2/2 = 1
T coordinate = 2/2 = 1
</pre>
</p>
<p>This is shown in the diagram below.
<br />
<img src="../images/texcoords.png" alt="Texture coordinates" />
</p>
Here is an example of a shader which will do this. First the vertex shader:
<pre>
attribute vec4 aVertex;
varying vec2 vTextureValue;

void main (void)
{
    gl_Position = aVertex;
    vTextureValue = vec2(0.5*(1.0 + aVertex.x), 0.5*(1.0 + aVertex.y));
}
</pre>
then the fragment shader:
<pre>
#extension GL_OES_EGL_image_external: require
precision mediump float;

varying vec2 vTextureValue;
uniform samplerExternalOES uTexture;

void main(void)
{
    gl_FragColor = texture2D(uTexture,vTextureValue);
}
</pre>
Note how this shader is taking in the vertices as an attribute variable (as before) - this would store the current vertex of the shape we're drawing. Also note the:
<pre>varying vec2 vTextureValue;</pre>
This represents the S and T coordinates of the texture that we want to map the current vertex to.  It is a <em>varying</em> variable: as we have seen, varying variables are used to pass information from the vertex to the fragment shader. 
So in the vertex shader we set the eye coordinate position to the vertex (assuming no view/projection transform) and then do the maths (as shown above) to calculate the texture coordinate from the vertex coordinates.</p>

<p>Then in the fragment shader we set the <code>gl_FragColor</code> to a colour taken from the texture.  Note the uniform variable <code>uTexture</code>, of the slightly cryptic type <code>samplerExternalOES</code>, represents the actual texture itself. A <em>sampler</em> is an object which samples pixels from the texture and projects them onto the shape we're drawing. We use the <code>texture2D</code> function to pull the correct drawing colour at the current S and T coordinate (which we calculated from the vertex position in the vertex shader) from the texture image.</p>

<h3>Linking the texture ID to the shader</h3>
<p>We need to link texture unit 0 (see above) to the shader variable representing the texture, <code>uTexture</code>:
<pre>
val refTextureUnit = gpuTexture.glGetUniformLocation("uTexture")
gpuTexture.setUniformInt(refTextureUnit, 0)
</pre>
<code>gpuTexture</code> is a <code>GPUInterface</code>  object representing the texture shaders. This code (which should come at the end of <code>onSurfaceCreated()</code>) is similar to what you have seen before: we first get a reference to the shader variable, and then set it to 0, for GPU texture unit 0.  This will associate GPU texture unit 0 with the <em>uTexture</em> shader variable.</p>
<h2>Linking the OpenGL texture and the camera</h2>
<p>We have considered how to create textures in general, but we have not yet looked at how to link textures to the camera. Our aim is to stream the camera feed (obtained using CameraX) into our texture. To do this we make use of a <code>SurfaceTexture</code> object. <code>SurfaceTexture</code>s are used for this purpose: to stream graphics into an OpenGL texture.</p>
<p>The relation between the different components of the system are shown below:
<br />
<img src="../images/camera_opengl_texture.png" alt="Relation between camera and OpenGL textures via SurfaceTexture" />
</p>
<h3>Creating our SurfaceTexture in our OpenGL code</h3>
<p>We create the surface texture from our texture ID:
<pre>cameraFeedSurfaceTexture = SurfaceTexture(textureId)</pre>
</p>
<h3>Sending the SurfaceTexture to the main activity</h3>
<p>Once we've created a <code>SurfaceTexture</code> from our texture ID, we send it to the main activity, so that the main activity can use it to stream the camera into. This might be done via a callback function, specified as a parameter to the <code>Renderer</code>:
<pre>class OpenGLRenderer(val textureAvailableCallback: (SurfaceTexture) -&gt; Unit) : GLSurfaceView.Renderer</pre>
So we pass our <code>SurfaceTexture</code> to this callback once we've created it:
<pre>textureAvailableCallback(cameraFeedSurfaceTexture!!)</pre>
Note that the <code>!!</code> states that we know, in this case, that the texture will never be null. (The <code>SurfaceTexture</code> in this case would be likely declared as a nullable, because it should be null before it's setup).
</p>
<h3>Receiving the texture in the main activity</h3>
<p>The main activity would typically store the surface texture as an attribute, which would be initialised by the callback function. The surface texture is then used as a destination by the CameraX code, as described below.</p>
<h3>Receiving the surface texture from the CameraX code</h3>
<p>Here is a version of the CameraX code (first covered in week 6) to work with a <code>SurfaceTexture</code>. The changed section is highlighted.
<pre>
private fun startCamera(): Boolean {
    if (checkPermissions()) {
        val cameraProviderFuture = ProcessCameraProvider.getInstance(this)
        cameraProviderFuture.addListener({
            val cameraProvider: ProcessCameraProvider = cameraProviderFuture.get()
            val preview = Preview.Builder().build().also {
                <strong>
                val surfaceProvider: (SurfaceRequest) -&gt; Unit = { request -&gt;
                    val resolution = request.resolution
                    surfaceTexture?.apply {
                        setDefaultBufferSize(resolution.width, resolution.height)
                        val surface = Surface(this)
                        request.provideSurface(
                            surface,
                            ContextCompat.getMainExecutor(this@MainActivity.baseContext))
                        { }

                    }
                }
                it.setSurfaceProvider(surfaceProvider)
                </strong>
            }

            val cameraSelector = CameraSelector.DEFAULT_BACK_CAMERA

            try {
                cameraProvider.unbindAll()
                cameraProvider.bindToLifecycle(this, cameraSelector, preview)

            } catch (e: Exception) {
                Log.e("OpenGL01Log", e.stackTraceToString())
            }
        }, ContextCompat.getMainExecutor(this))
        return true
    } else {
        return false
    }
}
</pre>
Previously we used our <code>PreviewView</code> object as the destination to stream the camera feed into with <code>setSurfaceProvider()</code>, Now, however we are going to use our <code>SurfaceTexture</code> (<code>surfaceTexture</code>) as the destination. 
</p>
<p>This is what the highlighted code is doing. Specifically we have to create a <em>surface provider</em> which is a lambda function which receives a request (the <code>request</code> parameter) from the camera, and obtains a <code>Surface</code> (a destination for the camera feed) from the surface texture, which will be the request's target. If you look at the highlighted code you can see that <code>surfaceProvider</code> is this lambda function, and the lambda function creates a <code>Surface</code> object from the surface texture and provides it to the request.</p>
<h2>Drawing the texture</h2>
<h3>Creating a texture rectangle</h3>
<p>You need to create an OpenGL rectangle covering the whole screen, and texture this rectangle using the camera. First of all You need to create a vertex buffer for this rectangle and corresponding index buffer, this would be done in <code>onSurfaceCreated()</code>. The vertices would be <code>(-1,1,0)</code>, <code>(-1,-1,0)</code>,<code>(1,-1,0)</code> and <code>(1,1,0)</code> and the indices <code>0,1,2,2,3,0</code>. As we have not specified a projection matrix this will draw two triangles (making up a rectangle) covering the <em>whole screen</em>. When there is no sense of perspective, <code>z</code> is always zero; <code>x</code> ranges from -1 (left of screen) to 1 (right of screen) and <code>y</code> ranges from -1 (bottom of screen) to 1 (top of screen).</p>
<h3>The onDrawFrame() method: drawing the texture</h3>
<p>
In <em>onDrawFrame()</em>, you draw the two triangles
to cover the whole screen. As we saw above, you should draw them unprojected, without either the view or the projection matrix.</p>
</p>
<p>Also in <em>onDrawFrame()</em>,
<strong>you need to call the SurfaceTexture's updateTexImage() method</strong>,
to update the SurfaceTexture with the latest camera frame, e.g.
<pre>
GLES20.glClear(GLES20.GL_COLOR_BUFFER_BIT or GLES20.GL_DEPTH_BUFFER_BIT)

// Update the surface texture with the latest frame from the camera
cameraFeedSurfaceTexture?.updateTexImage()

// draw your texture triangles here..

</pre>
</p>

<h3>Finally - set your activity's orientation to landscape</h3>
<p>Augmented reality applications are more usable in landscape mode.
It's also worth stopping the activity restarting on screen rotation.
This can be done as follows in the <em>AndroidManifest.xml</em>:
<pre>
&lt;activity android:name="com.example.MainActivity"
  android:screenOrientation="landscape"
  android:configChanges="orientation|keyboardHidden|screenSize"&gt;
</pre>
</p>

<h2>Exercise 1</h2>

<p>Add code to your existing app to show the camera feed on the OpenGL view as a texture. Make sure the CAMERA permission is added to your manifest. <strong>Does it work as expected? We will discuss this in class; as soon as we have, I will add the discussion to the notes.</strong></p>
<!--
<p>You will find the image comes out <em>upside down</em>. This is because images are arranged in memory from
top to bottom, but the T coordinate of a texture increases from bottom to top.
Change your shader to fix this!</p>
-->

<h2>Exercise 2 - Overlaying OpenGL 3D objects on an OpenGL camera-feed texture</h2>

<p>After completing Exercise 1, your 3D shapes will disappear. How can we fix this? An augmented-reality application will need to overlay a regular 3D view on top of a camera-feed texture.
How is this done?
</p>

<ul>
<li>Have separate shaders for the camera feed and for your regular OpenGL scene. Make sure you select the texture's <code>GPUInterface</code> before using it:
<pre>gpuTexture.select()</pre>
</li>
<li>You first draw your texture. Remember this does not need a view or projection matrix.
Note that OpenGL has inbuilt <em>depth testing</em>. Depth testing invoves checking whether objects are in front of others, and only drawing an object if it is currently visible. This can cause problems when using a camera texture, as the camera texture will be closer to the OpenGL camera than any 3D objects (as its <code>z</code> coordinate is 0), so by default it will hide those 3D objects. So, before drawing the camera feed, you need to disable depth testing.
<pre>GLES20.glDisable(GLES20.GL_DEPTH_TEST)</pre>
</li>
<li>After disabling the depth testing, draw the camera feed.</li>
<li>Enable depth testing again:
<pre>GLES20.glEnable(GLES20.GL_DEPTH_TEST)</pre>
</li>
<li>Switch to using your regular shader with <code>gpu.select()</code>, passing in the shader program ID as parameter e.g.:
<pre>gpu.select()</pre>
</li>
<li>Initialise the view matrix to an identity matrix, do any rotation and translation required, and pass the view and projection matrices to your regular shader, as normal.</li>
<li>Render your 3D scene as normal.</lik>
</ul>

<h2>Loading textures from file</h2>
<p>You might also want to texture <em>individual shapes</em> with textures loaded in from <em>file</em>. The code below shows how you can achieve this:
<pre>

val texFile = "texture.png"
try {
    val textureId = OpenGLUtils.loadTextureFromFile(ctx.assets, texFile) 
    if (textureId != 0) {
        // Now bind the texture ID to a texture unit as before, and send the texture unit to your shader...
    } else {
        // display error
    }
} catch(e: Exception) {
    // handle IOException if texture.png is not found
}
</pre>    
How is the <code>OpenGLUtils.loadTextureFromFile()</code> method working behind the scenes?
<ul>
<li>It creates a texture ID, as before.</li>
<li>It set up <em>magnification and minification filters</em>. For information on this, please see <a href='https://nwcourses.github.io/COM620/week3.html'>here</a>.</li>
<li>It then opens an image from the app's <code>assets</code>.</li>
<li>It then decodes the bitmap (decompress the PNG format into a raw bitmap) andassociates it with the currently active texture (the texture ID we allocated earlier).</li>
<li>Finally, once the bitmap has been loaded into the texture, it frees memory by recycling it.</li>
</ul>
</p>
</body>
</html>

