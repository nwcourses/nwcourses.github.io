<!DOCTYPE html>
<html>
<head>
<title>OpenGL Textures and the Camera</title>
<link rel='stylesheet' type='text/css' href='../css/android.css' />
<style>
img {
    border: 1px solid black;
}
</style>
</head>
<body>
<div class='titlebox'>
<h1>Mobile Development and 3D Graphics - Part 11</h1>
<h1>OpenGL part 5 - Textures and the Camera</h1>
</div>
<h2>Introduction</h2>

<p>A <em>texture</em> is a raster (bitmap) image which is overlaid on a 3D shape in OpenGL and other 3D graphics APIs. Textures are typically used to apply realistic surfaces to 3D models and shapes, for example a brick texture can be overlaid on a wall, or a metallic texture could be overlaid on a robot model. Textures can be a range of formats including PNG: in a typical application we would load the texture from file and apply it to a model or 3D shape in our OpenGL world. In this module, however, we are going to particularly focus on how textures can be used in an augmented reality (AR) application, to show the camera feed within an OpenGL scene.</p>

<h3>Components of an augmented reality OpenGL application</h3>
<p>The following classes are key to an OpenGL AR application:
    <ul>
    <li>The GLSurfaceView with its Renderer, the same as for any other OpenGL application;</li>
    <li>The camera management code (CameraX here) (week 6)</li>
    <li>A <code>SurfaceTexture</code>. This is an object representing the actual     texture obtained from the camera feed. You create it in your OpenGL code and setup CameraX to stream the camera feed to it.</li>
    <li>Sensor-handling code (week 5), with the orientation matrix used as the view matrix (we will look at this next week)</li>
    </ul>
</p>

<h2>Creating a texture in the OpenGL view</h2>

<h3>Initialising a texture ID</h3>

<p>
In our OpenGL renderer, we need to <em>create a texture</em>.
We use <code>GLES20.glGenTextures()</code> to do this. This typically goes in the <code>onSurfaceCreated()</code>:
<pre>
val textureId = IntArray(1)
GLES20.glGenTextures(1, textureId, 0)
</pre>
What this is doing is <em>generating a texture ID</em>. A texture ID is a unique ID number representing a particular texture. To discuss the code in detail, we first create a one-member array to hold the texture ID: <pre>val textureId = IntArray(1)</pre>
then generate the texture ID:
<pre>GLES20.glGenTextures(1, textureId, 0)</pre>
then, as long as the texture ID allocated is not 0 (which indicates an error),
we initialise the texture (this code is also in <code>onSurfaceCreated()</code>):
<pre>
if (textureId[0] != 0) {
    GLES20.glActiveTexture(GLES20.GL_TEXTURE0)
    GLES20.glBindTexture(GL_TEXTURE_EXTERNAL_OES, textureId[0])

    cameraFeedSurfaceTexture = SurfaceTexture(textureId[0])


    // Compile and link the shaders for textures (see below)

    // Create the vertex and index buffers for the texture rectangle (see below)

    // Call the texture callback (see below)
} 
</pre>
We select the active <em>GPU texture unit</em> and then bind the texture ID to it.  Note the <code>GL_TEXTURE0</code> refers to the first on-GPU <em>texture unit</em> (ref: <a href='https://www.khronos.org/opengles/sdk/1.1/docs/man/glActiveTexture.xml'>here</a>); this is <em>different</em> to the texture ID which is a CPU (Kotlin) based value.  There are multiple texture units, <code>GL_TEXTURE1</code> is the second, <code>GL_TEXTURE2</code> is the third, etc.
So, to summarise, this code is associating the GPU texture unit 0 with the texture ID we generated earlier.
</p>

<h3>Setting up texture objects</h3>

<p>Once we have initialised a texture, we need to specify how it will be applied to a shape.This is done on the GPU using shaders, as described below.</p>
<h3>S and T coordinates</h3>
<p>
Textures are defined using so-called S and T coordinates.
A texture is a 2D raster/bitmap image which is overlaid on a 3D shape. 
The S coordinate of the texture represents its horizontal axis, and the T
coordinate represents its vertical axis. The coordinates range from 0 to 1.
S=0, T=0 represents the bottom left. So:
    <ul>
    <li>S=0, T=0 : bottom left of texture;</li>
    <li>S=1, T=0 : bottom right of texture;</li>
    <li>S=1, T=1 : top right of texture;</li>
    <li>S=0, T=1 : top left of texture.</li>
    </ul>
When applying a texture to a 3D shape, we need to write a shader which
<em>relates x, y and z coordinates of a shape</em> to <em>the S and T coordinates of a texture</em>.
So we can specify what vertices of a shape the texture overlays. In the case of a camera feed, we need to create a <em>rectangle made up of two triangles</em>. This rectangle will occupy the whole screen and be flat, with no sense of depth and no perspective effect (no projection matrix applied). When this is the case, <strong>the x eye coordinate ranges from -1 (left of screen) to +1 (right of screen) and the y eye coordinate ranges from -1 (bottom of screen) to +1 (top of screen) with the origin in the centre. Furthermore z is always 0 as there is no sense of perspective.</strong> So the rectangle would have coordinates (-1,-1,0), (-1,1,0), (1,1,0) and (1,-1,0), and the following equation would relate <code>x</code> and <code>y</code> coordinates to <code>s</code> and <code>t</code> coordinates: 
<pre>
S coordinate = (X coordinate+1) / 2
T coordinate = (Y coordinate+1) / 2
</pre>
For example, for the vertex (-1, -1, 0):
<pre>
S coordinate = 0/2 = 0
T coordinate = 0/2 = 0
</pre>
and for the vertex (1, 1, 0):
<pre>
S coordinate = 2/2 = 1
T coordinate = 2/2 = 1
</pre>
</p>
<p>This is shown in the diagram below.
<br />
<img src="../images/texcoords.png" alt="Texture coordinates" />
</p>
Here is an example of a shader which will do this:
<pre>
val texVertexShader = "attribute vec4 aVertex;\n" +
                   "varying vec2 vTextureValue;\n" +
                   "void main (void)\n" +
                   "{\n" +
                   "gl_Position = aVertex;\n" +
                   "vTextureValue = vec2(0.5*(1.0 + aVertex.x), 0.5*(1.0 + aVertex.y));\n" +
                   "}\n"

val texFragmentShader = "#extension GL_OES_EGL_image_external: require\n" +
                   "precision mediump float;\n" +
                   "varying vec2 vTextureValue;\n" +
                   "uniform samplerExternalOES uTexture;\n" +
                   "void main(void)\n" +
                   "{\n" +
                   "gl_FragColor = texture2D(uTexture,vTextureValue);\n" +
                   "}\n";
</pre>
Note how this shader is taking in the vertices as an attribute variable (as before) - this would store the current vertex of the shape we're drawing. Also note the:
<pre>varying vec2 vTextureValue;</pre>
This represents the S and T coordinates of the texture that we want to map the current vertex to.  It is a <em>varying</em> variable: as we have seen, varying variables are used to pass information from the vertex to the fragment shader. 
So in the vertex shader we set the eye coordinate position to the vertex (assuming no view/projection transform) and then do the maths (as shown above) to calculate the texture coordinate from the vertex coordinates.</p>

<p>Then in the fragment shader we set the <code>gl_FragColor</code> to a colour taken from the texture.  Note the uniform variable <code>uTexture</code>, of the slightly cryptic type <code>samplerExternalOES</code>, represents the actual texture itself. We use the <code>texture2D</code> function to pull the correct drawing colour at the current S and T coordinate (which we calculated from the vertex position in the vertex shader) from the texture image.</p>

<h3>Linking the texture ID to the shader</h3>
<p>We need to link texture unit 0 (see above) to the shader variable representing the texture, <em>uTexture</em>:
<pre>
val refShaderVar = GLES20.glGetUniformLocation(shaderProgram, "uTexture")
GLES20.glUniform1i(refShaderVar, 0)
</pre>
This code (which should come at the end of <code>onSurfaceCreated()</code>) is similar to what you have seen before: we first get a reference to the shader variable, and then set it to 0, for GPU texture unit 0.  This will associate GPU texture unit 0 with the <em>uTexture</em> shader variable.</p>
<p>The <code>1i</code> in <code>glUniform1i()</code> refers to the fact that the shader variable is an integer, and there's only one (compare <code>glUniform4fv()</code> for an array of 4 floats)</p>
<h2>Linking the OpenGL texture and the camera</h2>
<p>We have considered how to create textures in general, but we have not yet looked at how to link textures to the camera. Our aim is to stream the camera feed (obtained using CameraX) into our texture. To do this we make use of a <code>SurfaceTexture</code> object. <code>SurfaceTexture</code>s are used for this purpose: to stream graphics into an OpenGL texture.</p>
<p>The relation between the different components of the system are shown below:
<br />
<img src="../images/camera_opengl_texture.png" alt="Relation between camera and OpenGL textures via SurfaceTexture" />
</p>
<h3>Creating our SurfaceTexture in our OpenGL code</h3>
<p>We create the surface texture from our texture ID:
<pre>cameraFeedSurfaceTexture = SurfaceTexture(textureId[0])</pre>
</p>
<h3>Sending the SurfaceTexture to the main activity</h3>
<p>Once we've created a <code>SurfaceTexture</code> from our texture ID, we send it to the main activity, so that the main activity can use it to stream the camera into. This might be done via a callback function, specified as a parameter to the <code>Renderer</code>:
<pre>class OpenGLRenderer(val textureAvailableCallback: (SurfaceTexture) -&gt; Unit) : GLSurfaceView.Renderer</pre>
So we pass our <code>SurfaceTexture</code> to this callback once we've created it:
<pre>textureAvailableCallback(cameraFeedSurfaceTexture!!)</pre>
Note that the <code>!!</code> states that we know, in this case, that the texture will never be null. (The <code>SurfaceTexture</code> in this case would be likely declared as a nullable, because it should be null before it's setup).
</p>
<h3>Receiving the texture in the main activity</h3>
<p>The main activity would typically store the surface texture as an attribute, which would be initialised by the callback function. The surface texture is then used as a destination by the CameraX code, as described below.</p>
<h3>Receiving the surface texture from the CameraX code</h3>
<p>Here is a version of the CameraX code (first covered in week 6) to work with a <code>SurfaceTexture</code>. The changed section is highlighted.
<pre>
private fun startCamera(): Boolean {
    if (checkPermissions()) {
        val cameraProviderFuture = ProcessCameraProvider.getInstance(this)
        cameraProviderFuture.addListener({
            val cameraProvider: ProcessCameraProvider = cameraProviderFuture.get()
            val preview = Preview.Builder().build().also {
                <strong>
                val surfaceProvider: (SurfaceRequest) -&gt; Unit = { request -&gt;
                    val resolution = request.resolution
                    surfaceTexture?.apply {
                        setDefaultBufferSize(resolution.width, resolution.height)
                        val surface = Surface(this)
                        request.provideSurface(
                            surface,
                            ContextCompat.getMainExecutor(this@MainActivity.baseContext))
                        { }

                    }
                }
                it.setSurfaceProvider(surfaceProvider)
                </strong>
            }

            val cameraSelector = CameraSelector.DEFAULT_BACK_CAMERA

            try {
                cameraProvider.unbindAll()
                cameraProvider.bindToLifecycle(this, cameraSelector, preview)

            } catch (e: Exception) {
                Log.e("OpenGL01Log", e.stackTraceToString())
            }
        }, ContextCompat.getMainExecutor(this))
        return true
    } else {
        return false
    }
}
</pre>
Previously we used our <code>PreviewView</code> object as the destination to stream the camera feed into with <code>setSurfaceProvider()</code>, Now, however we are going to use our <code>SurfaceTexture</code> (<code>surfaceTexture</code>) as the destination. 
</p>
<p>This is what the highlighted code is doing. Specifically we have to create a <em>surface provider</em> which is a lambda function which receives a request (the <code>request</code> parameter) from the camera, and obtains a <code>Surface</code> (a destination for the camera feed) from the surface texture, which will be the request's target. If you look at the highlighted code you can see that <code>surfaceProvider</code> is this lambda function, and the lambda function creates a <code>Surface</code> object from the surface texture and provides it to the request.</p>
<h2>Drawing the texture</h2>
<h3>Creating a texture rectangle</h3>
<p>You need to create an OpenGL rectangle covering the whole screen, and texture this rectangle using the camera. First of all You need to create a vertex buffer for this rectangle and corresponding index buffer, this would be done in <code>onSurfaceCreated()</code>. The vertices would be <code>(-1,1,0)</code>, <code>(-1,-1,0)</code>,<code>(1,-1,0)</code> and <code>(1,1,0)</code> and the indices <code>0,1,2,2,3,0</code>. As we have not specified a projection matrix this will draw two triangles (making up a rectangle) covering the <em>whole screen</em>. When there is no sense of perspective, <code>z</code> is always zero; <code>x</code> ranges from -1 (left of screen) to 1 (right of screen) and <code>y</code> ranges from -1 (bottom of screen) to 1 (top of screen).</p>
<h3>The onDrawFrame() method: drawing the texture</h3>
<p>
In <em>onDrawFrame()</em>, you draw the two triangles
to cover the whole screen. As we saw above, you should draw them unprojected, without either the view or the projection matrix.</p>
</p>
<p>Also in <em>onDrawFrame()</em>,
<strong>you need to call the SurfaceTexture's updateTexImage() method</strong>,
to update the SurfaceTexture with the latest camera frame, e.g.
<pre>
GLES20.glClear(GLES20.GL_COLOR_BUFFER_BIT or GLES20.GL_DEPTH_BUFFER_BIT)

// Update the surface texture with the latest frame from the camera
cameraFeedSurfaceTexture?.updateTexImage()

// draw your texture triangles here..

</pre>
</p>

<h3>Finally - set your activity's orientation to landscape</h3>
<p>Augmented reality applications are more usable in landscape mode.
It's also worth stopping the activity restarting on screen rotation.
This can be done as follows in the <em>AndroidManifest.xml</em>:
<pre>
&lt;activity android:name="com.example.MainActivity"
  android:screenOrientation="landscape"
  android:configChanges="orientation|keyboardHidden|screenSize"&gt;
</pre>
</p>

<h2>Exercise</h2>

<p>Develop an app which shows the camera feed on the OpenGL view as a texture. Use this template from GitHub <code>https://github.com/nwcourses/TextureStarter</code> as a starting point.  Make sure the CAMERA permission is added to your manifest.</p>
<p>You will find the image comes out <em>upside down</em>. This is because images are arranged in memory from
top to bottom, but the T coordinate of a texture increases from bottom to top.
Change your shader to fix this!</p>

<h2>Overlaying OpenGL 3D objects on an OpenGL camera-feed texture</h2>

<p>
An augmented-reality application will need to overlay a regular 3D view on top of a camera-feed texture.
How is this done?
</p>

<ul>
<li>Have separate shaders for the camera feed and for your regular OpenGL scene.</li>
<li>You first draw your texture. Remember this does not need a view or projection matrix.
Note that OpenGL has inbuilt <em>depth testing</em>. Depth testing invoves checking whether objects are in front of others, and only drawing an object if it is currently visible. This can cause problems when using a camera texture, as the camera texture will be closer to the OpenGL camera than any 3D objects (as its <code>z</code> coordinate is 0), so by default it will hide those 3D objects. So, before drawing the camera feed, you need to disable depth testing.
<pre>GLES20.glDisable(GLES20.GL_DEPTH_TEST)</pre>
</li>
<li>After disabling the depth testing, draw the camera feed.</li>
<li>Enable depth testing again:
<pre>GLES20.glEnable(GLES20.GL_DEPTH_TEST)</pre>
</li>
<li>Switch to using your regular shader with <code>GLES20.glUseProgram()</code>, passing in the shader program ID as parameter e.g.:
<pre>GLES20.glUseProgram(shaderProgram)</pre>
</li>
<li>Initialise the view matrix to an identity matrix, do any rotation and translation required, and pass the view and projection matrices to your regular shader, as normal.</li>
<li>Render your 3D scene as normal.</lik>
</ul>

<h2>Exercise 2</h2>
  
<p>Try combining your OpenGL triangle (or, if you're feeling ambitious, box) drawing exercise with your camera feed.</p>

<h2>Loading textures from file</h2>
<p>You might also want to texture <em>individual shapes</em> with textures loaded in from <em>file</em>. The code below shows how you can achieve this:
<pre>
val texFile = "texture.png"
val textureId = IntArray(1)
GLES20.glGenTextures(1, textureId, 0)
if (textureId[0] != 0) {
    GLES20.glBindTexture(GLES20.GL_TEXTURE_2D, textureId[0])
    GLES20.glTexParameteri(GLES20.GL_TEXTURE_2D, GLES20.GL_TEXTURE_MIN_FILTER, GLES20.GL_NEAREST)
    GLES20.glTexParameteri(GLES20.GL_TEXTURE_2D, GLES20.GL_TEXTURE_MAG_FILTER, GLES20.GL_NEAREST)
    val options = BitmapFactory.Options().apply { inScaled = false }
    val inputStream = ctx.assets.open(texFile)
    val bmp = BitmapFactory.decodeStream(inputStream, null, options)
    GLUtils.texImage2D(GLES20.GL_TEXTURE_2D, 0, bmp, 0)
    bmp?.recycle()

    // Now bind the texture ID to a texture unit as before, and send the texture unit to your shader...
}

</pre>    
How is this code working?
<ul>
<li>We create a texture ID, as before, and set it as the active texture with <code>GLES20.glBindTexture()</code>.</li>
<li>The <code>GLES20.glTexParameteri()</code> calls set up <em>magnification and minification filters</em>. For information on this, please see <a href='https://nwcourses.github.io/COM620/week3.html'>here</a>.</li>
<li>We then open an image from the app's <code>assets</code>. <code>assets</code> is an optional folder within your project (within <code>app/src/main</code>) containing external assets your app needs, such as images.</li>
<li>We then decode the bitmap (decompress the PNG format into a raw bitmap) and, using <code>GLUtils.texImage2D()</code>, associate it with the currently active texture (the texture ID we allocated earlier). Finally, once the bitmap has been loaded into the texture, we free memory by recycling it.</li>
</ul>

</body>
</html>

