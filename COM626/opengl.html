<!DOCTYPE html>
<html>
<head>
<title>3D Graphics with OpenGL</title>
<link rel='stylesheet' type='text/css' href='../css/android.css' />
</head>
<body>
<div class='titlebox'>
<h1>Mobile Development and 3D Graphics - Part 7</h1>
<h1>Introduction to OpenGL on Android</h1>
</div>
<p><em>OpenGL</em> is the standard API for cross-platform 3D graphics.
With OpenGL you can create hardware-accelerated 3D graphics on a range of
operating systems, including Windows, Linux, Mac OS X and Android. OpenGL takes advantage of a machine's Graphics Processing Unit (GPU) for fast rendering. 
In OpenGL, 3D shapes are made up of of individual <em>triangles</em>, each of which has three <em>vertices</em> (points making up the triangle).</p>
<h2>Important OpenGL concepts</h2>
<h3>OpenGL coordinate system</h3>
<p>Since OpenGL is a 3D API, it uses three coordinates: x, y and z. It's 
important to realise that <em>x increases to the right, and y increases upwards</em>, This is 
different to the situation in 2D computer graphics, where y increases 
downwards - but is the same as in standard maths.</p>
<p>Because this is 3D graphics, we have a third axis, the z axis. You can
think of this as pointing out at you from the screen, so that positive z
coordinates are "in front of the screen" and negative z coordinates are
"behind the screen". This is shown in the diagram below. </p>
<p><img src="../images/defaultview.png" alt="OpenGL default view" /></p>
<h2>OpenGL and OpenGL ES</h2>
<p><em>OpenGL ES</em> is a version of OpenGL optimised for devices with
limited resources, such as mobile devices (although these days many mobile
devices have capabilities comparable with traditional desktop machines!)
OpenGL ES programming is slightly different to standard OpenGL:
it is somewhat more complex but more efficient. There are various versions of
OpenGL ES: we will use OpenGL ES 2.0 as it provides what we need.</p>
<p>The API is significantly different to standard OpenGL. Drawing shapes takes
on a different approach: rather than drawing each triangle individually,
all vertices making up a complex shape are sent direct to the graphics card
at once, in a <em>buffer</em>, for fast, efficient drawing.</p>
<h3>The world and eye coordinate systems</h3>
<p>When working with OpenGL, we have two coordinate systems. An OpenGL application typically comprises a virtual <em>world</em> which we can wander around, as a player in a game or a user of a VR or AR app. <em>World coordinates</em> define the position of objects within this virtual world. We also have the <em>eye coordinate</em> system. This describes the coordinates of objects <em>with respect to the user's current view of the world</em>, which may not be the same. We expand upon the difference below.</p>
<p>Typically, the coordinates of 3D objects in an OpenGL application will be specified in our code, or more commonly, in an external data source such as a file or the web.
The coordinates of these objects are with respect to our virtual <em>world</em>, and can be thought of as the &quot;true&quot; coordinates of the objects within our 3D world - thus they are <em>world coordinates</em>. They define the absolute position of each object in our virtual world - in that respect they are conceptually similar to latitude and longitude. However, our <em>view</em> of
the coordinates might be different. Imagine the origin of the 3D world is
the central room in a 3D game. We might not be at that position in the world, 
we might be in a completely different room, and furthermore,
we might not be looking down the negative z-axis as in the default view.
We might be looking in a direction aligned at 45 degrees to both the x and
z axes, as shown in the diagram below. Additionally, we might be on a slope,
in which case, the up direction does not correspond to the y axis.</p>
<p><img src='../images/fig4.png' alt='Rotated coordinate system'/></p>
<p>The current on-screen 3D coordinates of a given object, with respect to the user's current view, are the <em>eye coordinates</em> of that object. The eye coordinate (0,0,0) is always the user's position within the world. <strong>And in eye coordinates, we are always facing negative <code>z</code>, with <code>x</code> increasing to the right and <code>y</code> increasing upwards</strong>, even if we are not facing negative <code>z</code> in world coordinates.</p>
<p>When <em>rendering</em> 3D shapes, we use <em>eye coordinates</em>, because we want to render them with respect to the <em>current view</em>. Thus we have a problem, because the shapes will typically be stored in a file or on the web as <em>world coordinates</em>. So we have to define a <em>transformation</em> to convert the world coordinates to eye coordinates. This transformation has two components:
	<ul>
	<li>A translation, to translate the coordinates from the world system to the eye system.</li>
	<li>A rotation, to rotate the field of view from the direction currently being faced in world coordinates into facing negative z in eye coordinates.</li>
	</ul>
This is explored in the following section.
</p>
<h3>The camera</h3>
<p>In many 3D applications, we want to allow the user to navigate through a 3D world, e.g. (as we have seen) a game or a VR or AR app. In these cases, we define the objects making up the world (which might include 3D buildings, terrain, points of interest, etc) in our code as world coordinates. We then define a <em>camera</em>, which represents the user's position in the world and the direction in which they are facing. Such a camera would have a set of world coordinates defining its position within the world, as well as a rotation. Such a camera, representing the user's position, is known as a <em>first-person camera</em>; other types of camera also exist which present a view of the world from the perspective of something other than the user (e.g. from the perspective of a certain room in a game, or a non-player character).</p>
<p>The <em>world coordinates</em> of the camera would vary, as the camera moves round the world. However, because a first-person camera represents the user's position, its <em>eye coordinates</em> will <em>always</em> be x=0, y=0, z=0.</p>
<h4>Translating and rotating the camera</h4>
<p>As seen above, the camera has a position and rotation. In more detail, it has: 
	<ul>
	<li>A defined position in the world (cx, cy, cz), as world coordinates;</li>
	<li>A defined set of <em>rotations</em> about each of the three axes in our
	world (x, y and z)</li>
	</ul>
By default, the camera is placed at the world origin (cx=0, cy=0, cz=0) and 
faces negative z. In this default position, world coordinates are equal to
eye coordinates. However we can move the camera around the world (e.g. a
player moving around in a game) and rotate it so it faces a direction other 
than negative z. The diagram below (which ignores y coordinates, for 
simplicity, in other words it shows the <em>xz-plane</em> - the flat surface on which every point has y=0) illustrates two such operations:
	<ul>
	<li>Firstly, the camera is translated so that it is at position cx=0, cz=+2;
	</li>
	<li>Secondly, the camera is rotated by 90 degrees anticlockwise.</li>
	</ul>
</p>
<p>
<img src="../images/world_and_eye_coords_2.png" alt="Moving a camera around, showing the relation between world and eye coordinates" />
</p>
<p>We are going to examine how the <em>eye coordinates</em> of points A and B
change as we move the camera.
	<ul>
	<li>Initially, the eye coordinates of A and B are equal to their world
	coordinates, as the camera is at the default position;</li>
	<li>When we move the camera to cx=0, cz=+2, the <em>difference in z
	coordinates between the camera and the two points increases from 2 to 4.</em>. In other words, <em>with respect to the camera</em>, or in <em>eye coordinates</em> A and B both have z coordinates of -4. The camera is at cz=+2, and still facing negative z, so points A and B both have z coordinates of -4 as they are both in front of the camera and four units away in the <em>z</em> direction. The <em>world</em> z coordinates of A and B, by contrast, are still -2: world coordinates do not change unless the objects move to a different position within the world (e.g. a monster moving in a game)</li>
	<li>We then <em>rotate</em> the camera by 90 degrees anticlockwise. This is done <em>around the y-axis</em> as shown in the diagram below:
<br />
<img src="../images/plane.png" alt="xz-plane and rotating around the y-axis" />
<br />By doing this, the camera is now facing <em>-x</em> in <em>world</em> coordinates. However, in <em>eye</em> coordinates, the camera is still facing -z, as it always does this by definition.</li>
	<li>As a result, A and B are now both to the <em>right</em> of the camera, rather than in front of it. They are still both 4 units away from the camera, as the camera <em>position</em> has not changed (only its rotation), but since they are 4 units <em>to the right</em> of the camera (rather than in front), the <em>eye x coordinate</em> of both is +4 (rather than the eye z coordinate being -4 as was the case originally). Remember that by definition, in eye coordinates, positive x is always to the right.</li>
	<li>The <em>eye z coordinates</em> of A and B will now differ, as A is now in front of the camera and B is behind the camera. Consequently, A will have a negative eye z coordinate and B will have a positive eye z coordinate. As both A and B are two units distance along the eye z axis, it follows that the eye z coordinate of A will be -2 and the eye z coordinate of B will be +2.</li>
	</ul>
</p>
<h4>Don't get confused by the word "camera"!</h4>
<p>It's important to not get confused by the terminology here. The first-person OpenGL camera is <strong>not</strong> the same as the actual hardware camera used to take pictures on the device. When we move onto AR, we will use the term "hardware camera" to distinguish the physical camera from the OpenGL camera.</p>
<h3>World and Eye Coordinates Demo</h3>
<p>Will appear soon.</p>
<h3>Matrices</h3>
<p>Many of you will have come across <em>matrices</em> in the past, at
school or college. <em>Matrices</em> are two-dimensional arrays of numbers which are most commonly used to represent geometric transformations which can be applied to shapes (both 2D and 3D), such as rotation, translation, magnification, stretch and shear. There are also other, more specialised, applications of matrices in science and maths - but these are out of the scope of this module. We will not go into the maths of matrices in great detail, but you need to be aware that a matrix represents a geometric transformation of some kind, such as translation or rotation. You also need to be aware of various matrices which exist within OpenGL which are involved in the rendering process. We will expand upon matrices a little next week.</p>
<h4>The view matrix</h4>
<p>OpenGL defines a matrix which specifies the transformation between the
world coordinates and the eye coordinates.  This is called the <em>view matrix</em>.  Every time you do a translation or rotation in code, the view matrix
is multiplied by the matrix which defines that rotation/translation. Then
finally the coordinates specified in code (the world coordinates) are multiplied by the view matrix to give the coordinates with respect to the user's current view.</p> 
<h4>The projection matrix</h4>
<p>The other type of matrix which is used in OpenGL is the <em>projection matrix</em>. This transforms the coordinates of shapes to add a sense of perspective, in other words it makes nearby shapes appear larger and further-away shapes appear smaller. So, shapes with eye z coordinates close to 0 (e.g. -1, -2) will appear relatively large, whereas shapes with negative z coordinates further from 0 (-10, -20, etc) will appear smaller. Without the projection matrix, no perspective is applied and shapes would appear the same size irrespective of their eye z coordinate. <strong>In fact, the exercises this week will not involve the projection matrix, and therefore, even though you will be doing some OpenGL programming the shapes you draw will appear flat, without a sense of perspective.</strong></p>
<!--
<h4>More on Matrices</h4>
<p>A <em>matrix</em> is a "grid" of numbers representing a particular <em>transformation</em> in 2D or 3D space. Matrices can be applied to coordinates (defining shapes, for example) to change (transform) them in some way. These transformations include translations (moving a shape around the world), scaling (making a shape bigger or smaller in one or more axes), or rotations (rotating an object around the x, y or z axis). In the case of OpenGL, we are mostly interested in translations and rotations.</p>
<p>When applying matrices to points, we typically represent the points in <em>vector</em> form, so a point <code>(x,y,z)</code> would become:
<pre class='vector'>
x
y
z
</pre>
</p>
<p>
A <em>matrix</em> would, by contrast, be represented as a grid, such as:
<pre class='vector'>
1 0 0
0 1 0
0 0 1
</pre>
<br />
(in actual fact, matrices are shown as being surrounded by parentheses, but this representation is difficult to do in HTML).</p>
<h4>Mutliplying vectors and matrices</h4>
<p>There is a predefined formula for multiplying a vector by a matrix to produce an output vector, which is:
<pre>
a b c       x           ax + by + cz
d e f   *   y    =      dx + ey + fz 
g h i       z           gx + hy + iz
</pre>
Note how we multiply the rows by the columns. So the first component (x component) of the output vector will be the result of multiplying each term in the first row of the matrix with the corresponding term in the vector. The second component (y component) of the output vector will be the result of multiplying each term in the second row of the matrix with the corresponding term in the vector. And the third component (z component) of the output vector will be the result of multiplying each term in the third row of the matrix with the corresponding term of the vector.</p> 
<h4>The identity matrix</h4>
A matrix which has <em>no effect</em> on the input vector is called an <em>identity matrix</em>. An identity matrix has <code>1</code> values along the main diagonal from top left to bottom right, and zeros elsewhere. We can show that the identity matrix has no effect from the above equation, as a=1, e=1, i=1 and all other values in the matrix are 0:
<pre>
1 0 0       x           1x + 0y + 0z     x
0 1 0   *   y    =      0x + 1y + 0z  =  y
0 0 1       z           0x + 0y + 1z     z
</pre>
</p>
<h4>Questions</h4>
<p>What effect will these matrices have on a point (x,y,z) ?
<ul>
<li>
<pre class='vector'>
2 0 0
0 2 0
0 0 2
</pre>
</li>
<li>
<pre class='vector'>
3 0 0
0 1 0
0 0 1
</pre>
</li>
<li>Try applying each matrix to each point of a square with coords:
<pre>x=0 y=0 z=0
x=2 y=0 z=0
x=2 y=2 z=0
x=0 y=2 z=0</pre>
Try drawing the output shape in each case, on the same diagram as the original shape.</li>
</ul>
<h3>The view matrix</h3>
<p>OpenGL defines a matrix which specifies the transformation between the
world coordinates and the eye coordinates. 
This is called the <em>view matrix</em>.
Every time you do a translation or rotation in code, the view matrix
is multiplied by the matrix which defines that rotation/translation. Then
finally the coordinates specified in code are multiplied by the view 
matrix to give the coordinates with respect to the user's current view.</p> 
<p>The view matrix would be used in the situation above as follows:
    <ul>
    <li>If we want to allow the user to navigate through a 3D world and
    define the objects making up the world (which might include
    3D buildings, terrain, etc) in world coordinates, with a camera
    (the user's position) at world coordinates
    (cx, cy, cz), we need to define a view
    matrix to perform a translation by (-cx, -cy, -cz).
    because we want the <em>world</em> position (cx, cy, cz) to appear at the 
    <em>eye</em> position (0, 0, 0). (We will of course also need to deal with the rotation
    of the camera)</li>
    </ul>
</p>
<h3>OpenGL uses 4x4 matrices</h3>
<p>In 3D graphics it's commonplace to use <em>4x4</em> matrices rather than
3x3. The reason why this is, is that they allow you to perform both 
translations and rotations in on step.
The matrix has a fourth column in which the translation is specified.</p>
<p>
 For example 
the standard matrix for rotating about the z axis anticlockwise by a given 
angle A is:
<pre>
cos A    -sin A    0
sin A    cos A     0
0        0         1
</pre>
What if we wanted to do both a rotation and translation with a single matrix?
We can combine the two with a 4x4 matrix, such as :
<pre>
cos A   -sin A     0    dx
sin A   cos A      0    dy
0       0          1    dz
0       0          0    1
</pre>
This matrix will perform a rotation (anticlockwise) by the angle A, <em>and
</em> translate the coordinates by dx in the x direction, dy in the y 
direction and dz in the z direction. It also follows that the matrix:
<pre>
cos A   -sin A     0    0 
sin A   cos A      0    0 
0       0          1    0 
0       0          0    1
</pre>
will perform the rotation by angle A around the z axis, but not the 
translation (as dx,dy and dz are all 0), and
<pre>
1   0     0    dx 
0   1     0    dy 
0   0     1    dz 
0   0     0    1
</pre>
will do the translation by dx, dy and dz, but not the rotation (as the rotation
part of the matrix, i.e. top 3 columns and top 3 rows, is the 3x3 identity
matrix)
</p>
<p>The view matrix is the matrix which defines the transformation between
the coordinate system defined in your code and the coordinate system
displayed on the screen. It is a 4x4 matrix. Each time you do a translation
or rotation in your code, you multiply the view matrix by the matrix
which defines that translation or rotation. Therefore, the final on-screen
coordinate system will be obtained by applying all the translations and
rotations defined in your code to the in-code coordinates of your shapes
in turn.</p>
<h4>The perspective matrix</h4>
<p>In addition to the view matrix covered above, OpenGL has a
<em>perspective matrix</em> which represents the transformations necessary to
render the scene to take perspective into account. Our view of the 3D world
is distorted according to the current perspective matrix.</p>
<h2>Architecture of a Android OpenGL ES 2.0 application</h2>
<p>An Android OpenGL ES 2.0 application consists of:
    <ul>
    <li>Standard Android code to process user events and communicate
    with servers if necessary</li>
    <li><em>Shaders</em> running on the GPU to control rendering</li>
    </ul>
</p>
<p>Android OpenGL ES 2.0 applications thus take place partly on the CPU 
and partly in the GPU (via shaders). A Android OpenGL ES 2.0 application
generally works in this way:
    <ul>
    <li>Kotlin or Java responds to user events by changing variables
    holding the position of objects in the 3D scene, or the user's
    current position in the world and the direction they are facing;</li>
    <li>These changes are then communicated to shaders on the GPU which
    actually do the rendering</li>
    </ul>
</p>
-->
<h2>Shaders</h2>
<p><em>This discussion applies to OpenGL ES in general; not just Android OpenGL ES 2.0.</em></p>
<p>Android OpenGL ES 2.0 (and all OpenGL ES 2.0 implementations) require the use of 
<em>shaders</em>. Shaders are small programs, written in a C-like language,
<em>GLSL</em> (GL Shader Language),
which run on the graphics card (GPU) and specify how vertices, and thus
shapes, appear on the screen (position, colour, lighting, textures etc).
A shader-based OpenGL application will consist of a CPU based program plus
a series of shaders running on the GPU. The CPU program passes information
to the shaders through a process known as the <em>rendering pipeline</em>.</p>
<h3>Types of shader</h3>
<p>There are two types of shader:
    <ul>
    <li><em>Vertex shaders</em>: specify where vertices appear on the screen, in particular, how they are transformed from world space to eye space and how perspective is applied.</li>
    <li><em>Fragment (or pixel) shaders</em>: determine how individual
    <em>pixels</em> appear on-screen (colour, lighting effects, etc)</li>
    </ul>
Vertex shaders run <em>before</em> fragment shaders in the rendering
process (called the <em>rendering pipeline</em>). This is illustrated in the diagram below:</p>
<p><img src="../images/renderingpipeline_java.png" 
alt="The rendering pipeline" /></p>
<h3>Shader variables</h3>
<p>There are three classes:
    <ul>
    <li><em>Attribute</em> variables: for quantities which vary for each
    vertex (e.g. vertex position)</li>
    <li><em>Uniform</em> variables: for quantities which remain the same
    per render (e.g. the matrix representing the current rotation/translation
    of the scene, the drawing colour)</li>
    <li><em>Varying</em> variables (to be covered later)</li>
    </ul>
</p>
<h4>Example of a vertex shader</h4>
<pre>
attribute vec4 aVertexPosition;
void main(void)
{
    gl_Position = aVertexPosition;
}
</pre>
<ul>
<li>This shader sets the position of the current vertex
(<em>gl_Position</em>; this is a built-in GLSL variable) to the
attribute variable <em>aVertexPosition</em>.</li>
<li>The attribute variable <em>aVertexPosition</em> (data type <em>vec4</em>;
this is an inbuilt GLSL data type) will contain the current position from the
vertex buffer;each vertex from the buffer in turn is sent to the vertex
shader.</li>
<li>In our Kotlin code we link the buffer with the <em>aVertexPosition</em>
variable, as we will see later.</li>
<li>This example does not apply the view or perspective matrix: we will look at these
below.</li>
</ul>
<h4>Example of a fragment shader</h4>
<p>
<pre>
void main (void)
{
    gl_FragColor = vec4(1.0, 0.0, 0.0, 1.0);
}
</pre>
<ul>
<li>
This is a simple fragment shader. All we do is set the colour of the current
fragment (area of pixels on the screen surrounding a given vertex) to red
(RGB 1,0,0; the fourth argument is the <em>alpha</em> - transparency -
component)</li>
<li><em>gl_FragColor</em> is an inbuilt GLSL variable representing the 
current fragment colour. Note the use of <em>vec4</em> again.</li>
</ul>
</p>

<!--
<h4>A vertex shader using the view and perspective matrices</h4>
<p>
<pre>
attribute vec4 aColour, aVertex;
uniform mat4 uMv, uPersp;

void main(void)
{
    gl_VertexPosition = uPersp*uMv*aVertex;

}
</pre>
<ul>
<li>Note how in this vertex shader we calculate the vertex position
on-screen (eye coordinates) by multiplying the input vertex position
(world coordinates; from the data model) by the view and
perspective matrices. We need to convert the input vertex position from
a 3-element to a 4-element vector due to the way the view and
perspective matrices are stored in memory (they are 4x4 matrices;
4x4 matrices allow the combination of a rotation and 
translation in one operation)</li>
-->
<h4> A fragment shader using a user-defined colour</h4>
<p>In this example we read in the colour from the uniform variable
<em>uColour</em>.
<pre>
precision mediump float;
uniform vec4 uColour;

void main(void)
{
    gl_FragColor = uColour;
}
</pre>
Note the three lines at the top, which specify that we are using high-precision floats.
This is needed if you pass across variable colours to the fragment shader.
</p>
<h2>Components of an Android OpenGL Application</h2>
<p>An Android OpenGL application includes the following components:
    <ul>
    <li>A <em>GLSurfaceView</em>. This is a View which will display your OpenGL scene.</li>
    <li>A class implementing the interface <em>GLSurfaceView.Renderer</em>, which contains your actual 3D rendering.
    This can either be the GLSurfaceView, or, if desired, a separate class to separate out the 3D rendering code from general code for managing a View. This contains a number of methods which must be implemented, which handle different aspects of the rendering process.</li>
    </ul>
</p>
<h3>Methods of GLSurfaceView.Renderer</h3>
<p>You need to provide implementations of these three methods.
<ul>
<li><em>onSurfaceCreated(unused: GL10, config: EGLConfig)</em> -
this runs when the OpenGL scene is first created. Setup code typically goes in here.</li>
<li><em>onDrawFrame(unused: GL10)</em> -
this runs each time we want to draw a new frame, e.g. when we update the
OpenGL scene with new objects or move around in the world.</li>
<li><em>onSurfaceChanged(unused: GL10, width: Int, height: Int)</em> -
this runs whenever the dimensions of the view are changed (e.g. change from
landscape to portrait)</li>
</ul>
</p>
    
<h2>Absolute basic example</h2>

<p>Here is an absolute basic example of an Android OpenGL application. It does not draw any graphics; it just shows you how you setup the OpenGL environment, and initialises a black screen ready for rendering 3D content. First we would have a main activity, as always:
<pre>
package com.example.opengl

import androidx.appcompat.app.AppCompatActivity
import android.os.Bundle

class MyActivity: AppCompatActivity() {
    override fun onCreate(savedInstanceState: Bundle) {
        val glView = OpenGLView(this)
        setContentView(glView)
    }
}
</pre>
We create an object of class <code>OpenGLView</code> which extends from <code>GLSurfaceView</code> (see below) and make this the main view of our activity. Now we move on to the <code>OpenGLView</code> object:
<pre>
package com.example.opengl

import android.opengl.GLSurfaceView
import android.content.Context

class OpenGLView(ctx: Context)  :GLSurfaceView(ctx) {
    init {
        setEGLContextClientVersion(2) // specify OpenGL ES 2.0
        val renderer = OpenGLRenderer()
        setRenderer(renderer) // set the renderer for this GLSurfaceView
    }
}
</pre>
The <code>OpenGLView</code> class doesn't use a lot, besides specifying we are using OpenGL ES 2.0, and setting the renderer for the OpenGLView. The renderer here is a separate class, <code>OpenGLRenderer</code>, shown below:
<pre>
package com.example.opengl

import android.opengl.GLES20
import android.opengl.GLSurfaceView
import java.nio.ByteBuffer
import java.nio.ByteOrder
import java.nio.FloatBuffer
import javax.microedition.khronos.egl.EGLConfig
import javax.microedition.khronos.opengles.GL10

class OpenGLRenderer: GLSurfaceView.Renderer {
    // We initialise the rendering here
    override fun onSurfaceCreated(unused: GL10, config: EGLConfig) {
        // Set the background colour (red=0, green=0, blue=0, alpha=1) 
        GLES20.glClearColor(0.0f, 0.0f, 0.0f, 1.0f)

        // Enable depth testing - will cause nearer 3D objects to automatically
        // be drawn over further objects
        GLES20.glClearDepthf(1.0f)
        GLES20.glEnable(GLES20.GL_DEPTH_TEST)
    }

    // We draw our shapes here
    override fun onDrawFrame(unused: GL10) {
        GLES20.glClear(GLES20.GL_COLOR_BUFFER_BIT or GLES20.GL_DEPTH_BUFFER_BIT)
    }

    // Used if the screen is resized
    override fun onSurfaceChanged(unused: GL10, w: Int, h: Int) {
        GLES20.glViewport(0, 0, w, h)
    }
}

</pre>
Note here:
    <ul>
    <li>In the <code>init</code> block, we specify that we are using OpenGL ES version 2.0 with
    <code>setEGLContextClientVersion(2)</code> and then specify that the current object will act
    as the renderer with <code>setRenderer(this)</code>.</li>
    <li>In <em>onSurfaceCreated()</em> we set the background colour to black (red 0, green 0, blue 0 and alpha - i.e transparency - 1, indicating fully opaque)</li>
    <li>Also in <em>onSurfaceCreated()</em> we clear the <em>depth buffer</em> and turn depth testing on. The depth buffer is an OpenGL feature which stores the "depth" of objects, i.e. the distance from the viewing position. This allows OpenGL to automatically work out whether one object is behind another, and not draw it if so.</li>
    <li>In <em>onDrawFrame()</em>, all we do for the moment is blank the screen. Drawing will go here later.</li>
    <li>In <em>onSurfaceChanged()</em> we adapt our view to the new width and height of the screen (these might change by rotating the device from portrait to landscape or vice-versa).  Later on we will explore the perspective matrix: this basically sets the perspective of our view so that further-away objects appear smaller than nearby ones.</li>
    </ul>
    
    
<h3>Loading in shaders</h3>
<p>The code above obviously does not do anything. To get it to draw shapes, we
first of all need to define our vertex and fragment shaders. Typically you put your shaders inside Strings, as follows (it's a bit messy but standard practice, see
<a href="http://developer.android.com/training/graphics/opengl/draw.html">this
article on the Android developer site</a>). The string variables containing the shader source would be attributes of your renderer class.
<pre>
val vertexShaderSrc =
    "attribute vec4 aVertex;\n" +
        "void main(void)\n" +
        "{\n"+
        "gl_Position = aVertex;\n" +
        "}\n"

val fragmentShaderSrc =
    "precision mediump float;\n" +
        "uniform vec4 uColour;\n" +
        "void main(void)\n"+
        "{\n"+
        "gl_FragColor = uColour;\n" +
        "}\n"
</pre>

<p>You then need to <em>compile the shaders</em> into native GPU machine code. A method is a good way to do this, as you will need to compile each shader (the vertex shader and the fragment shader).
<pre>
fun compileShader(shaderType: Int, shaderCode: String) : Int {
    val shader = GLES20.glCreateShader(shaderType)
    GLES20.glShaderSource(shader, shaderCode)
    GLES20.glCompileShader(shader)
    return shader
}
</pre>
<code>shaderCode</code> is a String containing your shader code.
<code>shaderType</code> is a constant integer representing the shader type: it will be either <code>GLES20.GL_VERTEX_SHADER</code> or <code>GLES20.GL_FRAGMENT_SHADER</code>.</p>

<p>The method creates a shader of the given type. This gives us an int variable, <code>shader</code>, which is a numerical handle on the shader. This is returned from the method and we can use it to <em>link</em> the two shaders into a single shader <em>program</em>.</p>

<h4>Linking shader code</h4>

<p>Having compiled the vertex and fragment shaders, we then <em>link</em> them into a usable <em>shader program</em>. Again this is best done as a method:
<pre>
fun linkShader(vertexShader: Int, fragmentShader: Int) : Int{
    val shaderProgram=GLES20.glCreateProgram()
    GLES20.glAttachShader(shaderProgram, vertexShader)
    GLES20.glAttachShader(shaderProgram, fragmentShader)
    GLES20.glLinkProgram(shaderProgram)
    GLES20.glUseProgram(shaderProgram)
    return shaderProgram
}
</pre>
Here, <em>vertexShader</em> and <em>fragmentShader</em> are the <em>int</em> variables returned from <code>compileShader()</code>, above. So we create a shader program, attach the individual shaders to it, and link them. We then call <code>GLES20.glUseProgram()</code> to specify that we want to use this shader program, not another one (it is possible for a single OpenGL application to have several shader programs which we can switch between).</p>

<p>We would typically setup our shaders in <code>onSurfaceCreated()</code>, as we only need to do it once. Compiling and linking them each time we render a frame would be extremely inefficient.</p>


<h3>Setting up a vertex buffer</h3>
<p>
Remember from the discussion above that the vertex data needs to be sent to the GPU in a <em>buffer</em>.  Here is how to create a buffer:
<pre>
fun makeBuffer(vertices: FloatArray) : FloatBuffer {
    val bbuf : ByteBuffer = ByteBuffer.allocateDirect(vertices.size * Float.SIZE_BYTES)
    bbuf.order(ByteOrder.nativeOrder())
    val fbuf : FloatBuffer  = bbuf.asFloatBuffer()
    fbuf.put(vertices)
    fbuf.position(0)
    return fbuf
}
</pre>
We pass in, as a parameter, a <code>FloatArray</code> of the x, y and z components of each vertex, allocate a ByteBuffer to store raw bytes (size: the length of the array multiplied by the size of the <em>float</em> data type),
generate a float buffer from the byte buffer, add the array to the float
buffer and set the position to 0 (the start of the buffer).
</p>

<p>We would typically setup our buffers in <code>onSurfaceCreated()</code>, as we only need to do it once. Doing it each time we render a frame would be very inefficient.</p>

<h3>Drawing buffered data</h3>

<p>Having setup our buffers we need to draw it in <em>onDrawFrame()</em>.
To do this we need to <em>send the buffered data to the GPU</em> and
<em>tell the GPU about the format of our data</em>. How do we do that?</p>

<h4>Accessing the shader from Kotlin</h4>
<p>The next thing we need to do is to link our buffer data to a shader variable, so that the shader can process each vertex in the buffer in turn.
To be able to use a shader variable from Kotlin, we need to get
a "handle" on it to allow Kotlin to manipulate it, and then link this
"handle" to our vertex data. To obtain the "handle", we can use the method
<em>glGetAttribLocation()</em>, and then enable it
with <em>glEnableVertexAttribArray()</em>. 
The name of the shader variable needs to be passed to <em>gl.getAttribLocation()</em>.
Here is a code example, which
stores the handle in the variable <em>ref_aVertex</em> (note how we use
the <em>shaderProgram</em> variable which we created when we compiled and linked the shader).
<pre>
// Create a reference to the attribute variable aVertex
val ref_aVertex = GLES20.glGetAttribLocation(shaderProgram,"aVertex")

// Enable it
GLES20.glEnableVertexAttribArray(ref_aVertex)
</pre>
</p>
<p>We can also get a handle on uniform variables. Remember from the discussion above that a uniform variable is a variable whose values do not vary from vertex to vertex.  A good example of a uniform variable is a colour (assuming the shape is of a uniform colour). GLES20.glGetUniformLocation() works in exactly the same way as GLES20.glGetAttribLocation() e.g.
<pre>
val ref_uColour = GLES20.glGetUniformLocation(shaderProgram,"uColour")
</pre>
Having obtained a reference to the uniform variable from outside our shader, we
then need to send data to it.
The method GLES20.glUniform4fv() is one example of a group of related methods for
sending uniform variables to the shader. This specific method glUniform<em>4fv</em>
means that it expects a <em>f</em>loat <em>v</em>ector (i.e. array) with
<em>4</em> members (red, green, blue and alpha - i.e. transparency - components).
Here is an example. Note how we pass in the reference to the shader variable and the array we want
to send. 
<pre>val colour = floatArrayOf(0.0f, 0.0f, 1.0f, 0.0f)
GLES20.glUniform4fv (ref_uColour, 1, val, 0)</pre>
(The other arguments mean: 1=we are modifying one value, and 0=the offset in the input array to
the data we're interested in. See
<a href="http://www.khronos.org/opengles/sdk/docs/man/xhtml/glUniform.xml">the OpenGL documentation</a>).</p>
<p>
There are other similar methods for sending other types of data (e.g. int arrays, 3-member float arrays, etc) to a uniform variable in the shader.
</p>



<h4>Drawing the shapes</h4>
<p>Now onto the actual drawing itself.
Having placed the vertices in a buffer and obtained a handle on the
shader variable which will contain each vertex, we can now actually draw
the shape. <strong>Drawing a shape is performed by specifying a buffer
or buffers to use (we could have one buffer for vertices and another for
colours, for example) and then telling Android OpenGL ES 2.0 what format
the buffer data is in and what shader variable should receive the buffered
data.</strong>
Here is how to do this. 
<ul>
<li>We need to tell Android OpenGL ES 2.0 what format the data is in, and what
shader variable will receive the data. 
To do this, we use
<em>glVertexAttribPointer()</em>:
<pre>GLES20.glVertexAttribPointer(ref_aVertex, 3, GLES20.GL_FLOAT, false, 0, vertexBuffer)</pre>
This is an interesting function as it takes several arguments
(see 
<a href="http://www.opengl.org/sdk/docs/man/xhtml/glVertexAttribPointer.xml">
OpenGL</a> - this is the C version of the method but it takes the same arguments as the Java/Kotlin version - and <a href="http://blog.tojicode.com/2011/05/interleaved-array-basics.html"> this blog article</a>). The function allows us to be highly flexible with our data, as we will see below:
    <ul>
    <li>Firstly, the handle on the shader attribute variable - this is needed
    as we need to specify the shader variable to send the vertex data to;</li>
    <li>Secondly, the number of elements per vertex (here, 3 as there are
    x, y and z coordinates);</li>
    <li>Thirdly, the type of data: here we are dealing with FLOAT variables
    (32-bit floating point numbers);</li>
    <li>The fourth argument, whether to normalise the data,
    can be generally left as <em>false</em>;</li>
    <li>Next is the <em>stride</em>. This is the number of bytes in between
    each record (each vertex here). Each vertex will occupy
    the number of components
    per vertex (3) multiplied by the number of bytes per component (4 since
    we are dealing with FLOATs), i.e. 12. However, here we have used the 
    value 0 rather than 12. Why is this?
    Zero is a special value
    to indicate that each vertex coordinate record is immediately followed by
    another vertex coordinate
    record. Would this not always be the case? It need not
    be: it would be possible to have a single buffer
    containing not just x,y,z coordinate data for each vertex but also other 
    data (e.g. the RGB colour of the current vertex),
    in which case the stride - the difference in bytes 
    between the start of the first vertex and the next  - would be
    greater than 12 as each record would contain not only the 12 bytes for
    the vertex coordinates but also additional data. In such a case, any data
    for each record 
    after the initial 12 bytes would be discarded as we have told
    <em>glVertexAttribPointer()</em> with the second and third arguments
    that we have 3 components per vertex
    and each component occupies 4 bytes. This is shown below.
    <br />
    <img src="../images/stride.png" 
    alt="Tightly-packed and non-tightly-packed
    vertex data" /></li>
    <li>Lastly is the vertex buffer containing the data we want to draw.</li> 
    </ul>
</li>
<li>Finally we draw the triangle. This works by passing each vertex in the
selected buffer to the shader in turn, loading each vertex into the 
currently-selected shader variable:
<pre> GLES20.glDrawArrays(GLES20.GL_TRIANGLES, 0, 3)</pre>
The second argument <em>0</em> is the index of the first vertex, and the 
third argument <em>3</em> is the number of vertices in total.</li>
</ul>
</p>
<h3>Extending the example to drawing two triangles</h3>
<p>The previous example could be extended very easily to draw two triangles.
The only differences are that we would fill the buffer with 6 points rather
than 3, and change the gl.drawArrays() call to reflect this:
<pre>GLES20.glDrawArrays(GLES20.GL_TRIANGLES, 0, 6)</pre>
Because we are in triangle-drawing mode (GLES20.GL_TRIANGLES), the 
glDrawArrays() call will know to treat each set of three vertices as
a separate triangle.</p>
<p>Another example:
<pre>GLES20.glDrawArrays(GLES20.GL_TRIANGLES, 3, 3)</pre>
If the buffer had at least 6 vertices (i.e. at least 2 triangles), this would
draw <em>the second triangle only</em> because the start vertex is 3 (the first vertex
of the second triangle), and the number of vertices to draw is 3.</p>


<h2>Exercise</h2>

<p>We are going to develop a simple OpenGL application to draw first one, then
two red triangles. </p>

<ol>
<li>Start with the template above, containing an activity, a GLSurfaceView and a Renderer.</li>
<li>Add the methods provided above to compile and link the shaders, and to initialise a vertex buffer using a float array of coordinates.</li>
<li>Add attributes to your Renderer:
	<ul>
	<li><code>shaderProgram</code> : an <code>Int</code>, containing a reference to the shader program. Initialise it to -1. If the shader program is created successfully, it will be a positive integer, so later on you can test whether the shader program was compiled and linked successfully by testing that this variable is at least 1.</li>
	<li><code>vertexBuffer</code> : a nullable <code>FloatBuffer</code>. This will contain the vertices when loaded.</li>
	<li>Two strings, one for each shader source. The shader code is provided above.</li>
	</ul>
</li>
<li>In <code>onSurfaceCreated()</code>, compile and link your shaders, and load the vertex buffer. The vertex buffer should contain these vertices:
<pre>x=0, y=0, z=0
x=1, y=0, z=0
x=0, y=1, z=0</pre>
</li>
<li>In <code>onDrawFrame()</code>, add the code to link your Kotlin variables (the buffer and the drawing colour) to the shader and draw the vertices (see the section "Drawing bufered data", above). The triangle should be drawn in red.</li>
<li>Modify your example to draw a second red triangle (as well as the first).
The second triangle should have the coordinates
<pre>x=0, y=0, z=0
x=-1, y=0, z=0
x=0, y=-1, z=0</pre>
</li>
<li>Make the second triangle appear in yellow.</li>
</ol>

</body>
</html>
