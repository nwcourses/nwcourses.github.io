<!DOCTYPE html><html><head><link rel='stylesheet' type='text/css' href='../css/dfti0910.css' /></head><body><h1 id="topic8efficientsearchingalgorithms">Topic 9: Further searching and searching algorithms</h1>
<h2 id="introduction">Introduction</h2>
<p>This week we will continue with our look at algorithms by examining some further algorithms, including <em>selection sort</em> and <em>insertion sort</em>. We will also cover approaches for searching <em>trees</em> (which we did earlier), including breadth-first search and depth-first-search. This topic will prepare us for looking at graphs in the final week.</p>
<h2>Further Sorting Algorithms</h2>
<h3 id="selectionsort">Selection sort</h3>
<p>Selection sort is another sorting algorithm which is conceptually simple but (relatively) inefficient. Nonetheless it has some advantages over bubble sort,
in particular, the number of swap operations is minimised (of order <code>O(n)</code> rather than <code>O(n^2)</code>), and will be used as another example of a sorting algorithm.</p>
<h4 id="howdoesselectionsortwork">How does selection sort work?</h4>
<p>We once again have an outer loop. This loops through each member of the list
(apart from the last). Within the outer loop we have an inner loop which 
compares the current member with each of the remaining members in turn. The
lowest remaining member is found, and if this is lower than the current
member, the current member and this lowest member is swapped.</p>
<p>See the diagram below, which is based from notes provided by my former colleague Brian Dup√©e, which were in turn sourced from the site sorting-algorithms.com (this site still exists, provided by Toptal, but does not appear to contain these resources anymore)</p>
<p><img src="images/selection_sort.png" alt="Selection Sort" /></p>
<p>It is of interest what this algorithm's time complexity equals, as it raises
an important conceptual point about the Big O notation. If you imagine 
sorting 5 values, what needs to be done?</p>
<ul>
<li>The outer loop needs to perform <code>n-1</code>, or 4 swaps.</li>
<li>The number of operations the inner loop needs to do, depends on where
we are in the outer loop. So for the first iteration of the outer loop, we
will need to loop through members 1-4 (starting at 0 for the first member) and
compare them with member 0. For the second iteration, we need to loop through
members 2-4 and compare them with member 1. For the third iteration, we loop
through members 3 and 4 and compare them with member 2, and for the fourth
iteration, we compare member 4 with member 3.</li>
</ul>
<p>So the total number of operations for a 5-member list is 4+3+2+1, or more generally for an <code>n</code>-member list, the number of operations is the sum of all the numbers from 1 to <code>n</code>. This value is equal to <code>(n + n^2) / 2</code>.</p>
<p>This is an equation of quadratic form, as it can be expressed instead as <code>0.5n^2 + 0.5n</code>. So with large <code>n</code> values the <code>n^2</code> term will dominate, and thus, this is an <code>O(n^2)</code> class of algorithm, i.e. less efficient. It is not quite as inefficient as bubble sort, particularly given that less swaps are involved (swapping is a relatively expensive operation as it requires a temporary variable to be created), but it can still potentially be improved.</p>
<p>A less mathematical way of thinking about it is to consider the fact that there are <em>two loops, one within another</em>. Algorithms of this form - even if one of the loops does not iterate through the entire list - will generally be <code>O(n^2)</code>.</p>
<p>When swapping numbers, swapping is not particularly computationally
expensive. However, if the two values are complex objects which take time to 
initialise, then the fact that we need to create a temporary variable can
mean a swap is expensive. So in cases like this, selection sort is particularly favoured over bubble sort.</p>
<h3 id="insertionsort">Insertion sort</h3>
<p>A third type of simple sort, with complexity <code>O(n^2)</code> in many cases, but occasionally <code>O(n)</code> (see below), is the <em>insertion sort</em>. This is shown on the diagram below.</p>
<p><img src="images/insertion_sort.png" alt="Insertion sort" /></p>
<p>Insertion sort works by progressively <em>inserting</em> values into the list at the
correct place. A typical implementation will work through a list and sort
it in-place (i.e. within the original list, rather than creating a new list), 
by defining a "divider" that separates out the list into 
sorted and unsorted parts. The "divider" moves forward one position on each
iteration of the outer loop. The "divider" is shown by a red box in the diagram.</p>
<p>Each time we move to a new "divider", we compare the "divider" with the sorted
part of the list, to its left. If all members of the sorted part are less than
the "divider", we do nothing. If on the other hand we find a member, or 
multiple members, of the sorted part of the list which are <em>greater</em> than the "divider", then we insert the "divider" into the sorted part at the appropriate place, and move all the remaining members on one place. This is done with the values <strong>28</strong> and <strong>62</strong> in the diagram above. This operation would be done using an inner loop, starting at the divider and moving back through the sorted part while there are values greater than the "divider". As soon as we find a value less than the "divider", we have our insertion position.</p>
<p>Note that we can perform a useful "trick" here which can prevent us having to do the inner loop at all. If the last value of the sorted part of the list - which will be the value immediately to the left (i.e one index below) the "divider", is <em>less</em> than the "divider", then we <strong>know</strong> that the "divider" is already in its correct position. This is because the sorted part of the list is sorted, and the highest value within the sorted part will be immediately to the left of the "divider". So if the "divider" is greater than this value, we do not to move it. Thus, we do not need an inner loop.</p>
<p><strong>You should always be on the lookout for "tricks" like this when designing algorithms. Is there something about the data which means that, in some cases, we can avoid having to perform computationally expensive operations like an additional loop, and therefore reduce the complexity of the algorithm?</strong></p>
<p>The consequence of this is that if the list is "almost sorted", then the complexity will be closer to <code>O(n)</code> than <code>O(n^2)</code>, as in most cases, we will not need an inner loop. So if, for whatever reason, we know that our list is almost sorted already, an insertion sort is a strongly favoured choice.</p>
<h2 id="treesearching">Tree searching</h2>
<p>We can efficiently search a <em>tree</em> by virtue of its structure. There are two 
general techniques for searching a tree:</p>
<ul>
<li><p><em>Depth-first search</em>. This uses the recursive tree traversal technique we have examined already in <a href="week6.html">Week 6</a>, starting at the root node and then
recursively descending into the child nodes. When the search term is found, we will return it. If the tree is not sorted, we need to keep track of which nodes have been visited and which have not, so we can explore the whole tree. </p></li>
<li><p><em>Breadth-first search</em>. Rather than recursively descending the tree, we
instead we consider each <em>level</em> one at a time. So if we have a tree 
organised as below:</p></li>
</ul>
<p><img src="images/bfs_tree.png" alt="Example tree for breadth-first search" /></p>
<p>we would first consider node A (the first level), then nodes B and C (the second level), then nodes D, E, F and G (the third level).</p>
<p>Breadth-first search has various applications in route-finding, so by covering it now, we will prepare ourselves for the graphs and route-finding topic in Week 10. </p>
<h3 id="breadthfirstsearch">Breadth-first search</h3>
<p>We will now look at breadth-first search in some detail. It is of note that breadth-first search does <em>not require recursion</em>.</p>
<p>Note the order again in which we consider our nodes: A (the root node) first, followed by B and C (the root's child nodes), followed by D, E, F, and G (the child nodes of B and C).</p>
<p>Our aim is to try and search for a value in the tree. How might we do this?
We check A first, adding it to a collection of nodes to be considered.</p>
<pre><code>[A]
</code></pre>
<p>If A doesn't match, we move on to its child nodes - B and C - and remove A from the collection of nodes to be considered. So, once we've considered and rejected A, the collection will contain B and C.</p>
<pre><code>[B C]
</code></pre>
<p>If B doesn't match, then we know we might need to consider <em>its</em> child nodes (D and E) so we add them to the collection of nodes. Because we are doing a <em>breadth first</em> search, though, and considering the tree "level by level", D and E must be added to the collection of nodes to be searched <em>after</em> C - because when we consider B, we will not have considered C yet, and C is on the same level as B. So after considering B, we remove it fron the collection of nodes to be considered - so we are left with:</p>
<pre><code>[C D E]
</code></pre>
<p>Next we consider C. If C doesn't match, then we know we might need to consider <em>its</em> child nodes (F and G) so we add them to the collection of nodes to be considered. Because we consider each level left-to-right, these should again be added to the collection <em>after</em> D and E. If C doesn't match, again we remove it, so at this stage, the collection of nodes to be considered will be:</p>
<pre><code>[ D E F G ]
</code></pre>
<p>Hopefully you can see that it has linear <code>O(n)</code> complexity, dependent on the number of nodes, and thus is not especially efficient when it comes to simple trees. The use of breadth-first search becomes more apparent when we look at graphs; I am introducing the algorithm here so that you understand it when we begin the graphs topic.</p>
<h4 id="whichdatastructuretouseforthenodestobeprocessed">Which data structure to use for the nodes to be processed?</h4>
<p>We use a <em>queue</em> here, because with the breadth-first search, we process the nodes in the order they are added. In other words, the node we add first is the node we process first. We can use the standard library collection <code>collections.deque</code> to do this. A <em>deque</em> is a double-ended queue: a queue which you can add to and remove from both the front and back of. However, here we are just using it as a regular queue.</p>
<p>Here is an example of using a deque as a regular queue. Note how we use <code>append()</code> to add to the deque, and <code>popleft()</code> to remove from it.</p>
<pre><code>import collections

q = collections.deque([])
q.append(123)
q.append(456)
q.append(789)
print(q.popleft())
print(len(q))
</code></pre>
<h3 id="usingtreestostorekeyvaluepairs">Using trees to store key/value pairs</h3>
<p>I have said several times that we test whether each node 'matches'. What exactly do we mean by this? We'd typically use a tree to store and efficiently search for data as an alternative to hash tables. In other words, each node would contain a key/value pair, such as "1smitj01"/"John Smith" for the key and value respectively in a student records system. In Python, this would probably be stored as a tuple (if we didn't want to change the values) or two-member list (if we did). So we could search the tree for a given key (index), and test each node we find in breadth-first (or depth-first) search to see if its key is the key we are searching for. If it is, then we return the value.</p>
<h3 id="binarysearchtreedepthfirst">Binary search tree (depth-first)</h3>
<p>This uses the technique we have seen already to search a tree for a given key (index), namely recursion. We recursively search each successive child node, starting at the root node, until we find the key we are looking for. </p>
<p>For the search to be efficient, we need the tree to be <em>sorted</em>. We have seen already in topic 6 how to add items to a tree in sorted order. The search technique can be considered a specific case of depth-first search in which the tree is sorted and thus we know whether to select the left or right child node, and can ignore the other. For general depth-first search, we cannot guarantee this, and we will need to "backtrack" along branches of the tree to ensure all nodes are visited. In fact depth-first search is a general technique that works on graphs - to be covered in week 10 - as well as trees. As we will see, graphs are usually not sorted in any way.</p>
<p>To search a sorted binary tree we need to:</p>
<ul>
<li>Start at the root node and test that first. Return the value if the key matches.</li>
<li>If the root node key is less than the key we are searching for, descend to the right node, otherwise descend to the left node.</li>
<li>Repeat the process recursively until the key matches; when that happens, return the value.</li>
</ul>
<p>Hopefully you can see that the complexity is of <code>O(log n)</code> form and it scales well to large values of <code>n</code>, the number of items stored in the tree.</p>
<ul>
<li>If the tree has one level (a root only), we search one node.</li>
<li>If the tree has two levels (three members), we search two nodes.</li>
<li>If the tree has three levels (seven members), we search three nodes.</li>
<li>If the tree has four levels (15 members), we search four nodes.</li>
</ul>
<p>The table below shows the relation between the number of nodes in a sorted and balanced binary tree, and the number of searches. (A <em>balanced</em> tree is one in which the data is evenly added to the left and right sides of the tree. There are techniques for ensuring this, but we will not have time to cover these in this module; I will leave this up to you to research).</p>
<table>
<thead>
<tr>
<th>Number of nodes</th>
<th>Number of searches</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>7</td>
<td>3</td>
</tr>
<tr>
<td>15</td>
<td>4</td>
</tr>
<tr>
<td>31</td>
<td>5</td>
</tr>
<tr>
<td>63</td>
<td>6</td>
</tr>
<tr>
<td>127</td>
<td>7</td>
</tr>
<tr>
<td>255</td>
<td>8</td>
</tr>
<tr>
<td>511</td>
<td>9</td>
</tr>
<tr>
<td>1023</td>
<td>10</td>
</tr>
</tbody>
</table>
<h2>Exercises</h2>
<p>I would advise using the first 90 minutes to do Exercise 1, and the second to do Exercise 2.</p>
<h3 id="exercise1">Exercise 1</h3>
<p>Implement <em>either</em> selection <em>or</em> insertion sort. Use pseudocode to help you. If you finish one, try the other.</p>
<h3 id="exercise2">Exercise 2</h3>
<p>Start with either your own tree from the trees topic (if you have completed the week 6 exercise) or the pre-written tree solution at <a href="https://raw.githubusercontent.com/nwcourses/nwcourses.github.io/master/COM421/solutions/tree.py">GitHub</a></p>
<ol>
<li>Implement <em>either</em> breadth-first <em>or</em> depth-first search using the tree (for now return True or False depending on whether the number is in the tree or not).</li>
<li>If you finish question 1, implement the other search.</li>
</ol>
<p>Again, you may find that <em>pseudocode</em> helps you.</p></body></html>
